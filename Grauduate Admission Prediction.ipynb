{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyforest\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('Admission_Predict_Ver1.1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Serial No.         500 non-null    int64  \n",
      " 1   GRE Score          500 non-null    int64  \n",
      " 2   TOEFL Score        500 non-null    int64  \n",
      " 3   University Rating  500 non-null    int64  \n",
      " 4   SOP                500 non-null    float64\n",
      " 5   LOR                500 non-null    float64\n",
      " 6   CGPA               500 non-null    float64\n",
      " 7   Research           500 non-null    int64  \n",
      " 8   Chance of Admit    500 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 35.3 KB\n"
     ]
    }
   ],
   "source": [
    "# Checking the information of the whole dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>250.500000</td>\n",
       "      <td>316.472000</td>\n",
       "      <td>107.192000</td>\n",
       "      <td>3.114000</td>\n",
       "      <td>3.374000</td>\n",
       "      <td>3.48400</td>\n",
       "      <td>8.576440</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.72174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>144.481833</td>\n",
       "      <td>11.295148</td>\n",
       "      <td>6.081868</td>\n",
       "      <td>1.143512</td>\n",
       "      <td>0.991004</td>\n",
       "      <td>0.92545</td>\n",
       "      <td>0.604813</td>\n",
       "      <td>0.496884</td>\n",
       "      <td>0.14114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>125.750000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>8.127500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>250.500000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>8.560000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.72000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>375.250000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>9.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.82000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.97000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Serial No.   GRE Score  TOEFL Score  University Rating         SOP  \\\n",
       "count  500.000000  500.000000   500.000000         500.000000  500.000000   \n",
       "mean   250.500000  316.472000   107.192000           3.114000    3.374000   \n",
       "std    144.481833   11.295148     6.081868           1.143512    0.991004   \n",
       "min      1.000000  290.000000    92.000000           1.000000    1.000000   \n",
       "25%    125.750000  308.000000   103.000000           2.000000    2.500000   \n",
       "50%    250.500000  317.000000   107.000000           3.000000    3.500000   \n",
       "75%    375.250000  325.000000   112.000000           4.000000    4.000000   \n",
       "max    500.000000  340.000000   120.000000           5.000000    5.000000   \n",
       "\n",
       "            LOR         CGPA    Research  Chance of Admit   \n",
       "count  500.00000  500.000000  500.000000         500.00000  \n",
       "mean     3.48400    8.576440    0.560000           0.72174  \n",
       "std      0.92545    0.604813    0.496884           0.14114  \n",
       "min      1.00000    6.800000    0.000000           0.34000  \n",
       "25%      3.00000    8.127500    0.000000           0.63000  \n",
       "50%      3.50000    8.560000    1.000000           0.72000  \n",
       "75%      4.00000    9.040000    1.000000           0.82000  \n",
       "max      5.00000    9.920000    1.000000           0.97000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## General statistics of the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial No.           0\n",
       "GRE Score            0\n",
       "TOEFL Score          0\n",
       "University Rating    0\n",
       "SOP                  0\n",
       "LOR                  0\n",
       "CGPA                 0\n",
       "Research             0\n",
       "Chance of Admit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking duplicated values\n",
    "\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In our Neural Network Architechture we will keep 1 input layer of 7 nodes, 1 Hidden Layer of 7 nodes and 1 Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant column\n",
    "\n",
    "df.drop(columns=['Serial No.'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the data for training and testing\n",
    "\n",
    "X = df.iloc[:,0:-1]\n",
    "y= df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "0          337          118                  4  4.5   4.5  9.65         1\n",
       "1          324          107                  4  4.0   4.5  8.87         1\n",
       "2          316          104                  3  3.0   3.5  8.00         1\n",
       "3          322          110                  3  3.5   2.5  8.67         1\n",
       "4          314          103                  2  2.0   3.0  8.21         0\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "495        332          108                  5  4.5   4.0  9.02         1\n",
       "496        337          117                  5  5.0   5.0  9.87         1\n",
       "497        330          120                  5  4.5   5.0  9.56         1\n",
       "498        312          103                  4  4.0   5.0  8.43         0\n",
       "499        327          113                  4  4.5   4.5  9.04         0\n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.92\n",
       "1      0.76\n",
       "2      0.72\n",
       "3      0.80\n",
       "4      0.65\n",
       "       ... \n",
       "495    0.87\n",
       "496    0.96\n",
       "497    0.93\n",
       "498    0.73\n",
       "499    0.84\n",
       "Name: Chance of Admit , Length: 500, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>310</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>318</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>300</td>\n",
       "      <td>101</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>300</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>322</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>307</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>321</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>326</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>300</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "238        310          104                  3  2.0   3.5  8.37         0\n",
       "438        318          110                  1  2.5   3.5  8.54         1\n",
       "475        300          101                  3  3.5   2.5  7.88         0\n",
       "58         300           99                  1  3.0   2.0  6.80         1\n",
       "380        322          104                  3  3.5   4.0  8.84         1\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "255        307          110                  4  4.0   4.5  8.37         0\n",
       "72         321          111                  5  5.0   5.0  9.45         1\n",
       "396        325          107                  3  3.0   3.5  9.11         1\n",
       "235        326          111                  5  4.5   4.0  9.23         1\n",
       "37         300          105                  1  1.0   2.0  7.80         0\n",
       "\n",
       "[400 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>313</td>\n",
       "      <td>106</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>312</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>339</td>\n",
       "      <td>119</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>316</td>\n",
       "      <td>107</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>325</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>327</td>\n",
       "      <td>111</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>322</td>\n",
       "      <td>114</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>298</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>316</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>305</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "304        313          106                  2  2.5   2.0  8.43         0\n",
       "340        312          107                  3  3.0   3.0  8.46         1\n",
       "47         339          119                  5  4.5   4.0  9.70         0\n",
       "67         316          107                  2  3.5   3.5  8.64         1\n",
       "479        325          110                  4  4.5   4.0  8.96         1\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "11         327          111                  4  4.0   4.5  9.00         1\n",
       "192        322          114                  5  4.5   4.0  8.94         1\n",
       "92         298           98                  2  4.0   3.0  8.03         0\n",
       "221        316          110                  3  3.5   4.0  8.56         0\n",
       "110        305          108                  5  3.0   3.0  8.48         0\n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to scale our data before applying in the neural network\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4       , 0.42857143, 0.5       , ..., 0.57142857, 0.50320513,\n",
       "        0.        ],\n",
       "       [0.56      , 0.64285714, 0.        , ..., 0.57142857, 0.55769231,\n",
       "        1.        ],\n",
       "       [0.2       , 0.32142857, 0.5       , ..., 0.28571429, 0.34615385,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.7       , 0.53571429, 0.5       , ..., 0.57142857, 0.74038462,\n",
       "        1.        ],\n",
       "       [0.72      , 0.67857143, 1.        , ..., 0.71428571, 0.77884615,\n",
       "        1.        ],\n",
       "       [0.2       , 0.46428571, 0.        , ..., 0.14285714, 0.32051282,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46      , 0.48      , 0.25      , 0.375     , 0.25      ,\n",
       "        0.44140625, 0.        ],\n",
       "       [0.44      , 0.52      , 0.5       , 0.5       , 0.5       ,\n",
       "        0.453125  , 1.        ],\n",
       "       [0.98      , 1.        , 1.        , 0.875     , 0.75      ,\n",
       "        0.9375    , 0.        ],\n",
       "       [0.52      , 0.52      , 0.25      , 0.625     , 0.625     ,\n",
       "        0.5234375 , 1.        ],\n",
       "       [0.7       , 0.64      , 0.75      , 0.875     , 0.75      ,\n",
       "        0.6484375 , 1.        ],\n",
       "       [0.42      , 0.28      , 0.25      , 0.375     , 0.625     ,\n",
       "        0.40625   , 1.        ],\n",
       "       [0.6       , 0.4       , 0.5       , 0.5       , 0.625     ,\n",
       "        0.5625    , 1.        ],\n",
       "       [0.74      , 0.36      , 0.5       , 0.75      , 0.75      ,\n",
       "        0.390625  , 1.        ],\n",
       "       [0.62      , 0.68      , 0.5       , 0.625     , 0.75      ,\n",
       "        0.59765625, 1.        ],\n",
       "       [0.56      , 0.48      , 0.25      , 0.75      , 0.75      ,\n",
       "        0.2421875 , 1.        ],\n",
       "       [0.48      , 0.52      , 0.25      , 0.375     , 0.75      ,\n",
       "        0.37890625, 0.        ],\n",
       "       [0.06      , 0.12      , 0.25      , 0.25      , 0.75      ,\n",
       "        0.1953125 , 1.        ],\n",
       "       [0.74      , 0.4       , 1.        , 0.5       , 0.625     ,\n",
       "        0.6015625 , 1.        ],\n",
       "       [0.74      , 0.8       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.671875  , 0.        ],\n",
       "       [0.62      , 0.6       , 0.5       , 0.625     , 0.625     ,\n",
       "        0.5859375 , 1.        ],\n",
       "       [0.68      , 0.72      , 1.        , 1.        , 1.        ,\n",
       "        0.6953125 , 1.        ],\n",
       "       [0.32      , 0.36      , 0.5       , 0.625     , 0.5       ,\n",
       "        0.35546875, 0.        ],\n",
       "       [0.46      , 0.6       , 0.5       , 0.75      , 0.625     ,\n",
       "        0.6640625 , 0.        ],\n",
       "       [0.82      , 0.84      , 1.        , 0.75      , 0.625     ,\n",
       "        0.8359375 , 1.        ],\n",
       "       [0.5       , 0.44      , 0.5       , 0.25      , 0.375     ,\n",
       "        0.4609375 , 0.        ],\n",
       "       [0.44      , 0.16      , 0.        , 0.625     , 0.5       ,\n",
       "        0.34375   , 1.        ],\n",
       "       [0.42      , 0.52      , 0.75      , 0.875     , 0.875     ,\n",
       "        0.6640625 , 1.        ],\n",
       "       [0.76      , 0.56      , 0.75      , 0.875     , 0.75      ,\n",
       "        0.734375  , 1.        ],\n",
       "       [0.16      , 0.28      , 0.75      , 0.375     , 0.875     ,\n",
       "        0.15234375, 1.        ],\n",
       "       [0.7       , 0.72      , 0.75      , 0.625     , 0.625     ,\n",
       "        0.6328125 , 0.        ],\n",
       "       [0.5       , 0.44      , 0.25      , 0.25      , 0.375     ,\n",
       "        0.13671875, 0.        ],\n",
       "       [0.82      , 0.72      , 1.        , 0.75      , 1.        ,\n",
       "        0.9765625 , 1.        ],\n",
       "       [0.36      , 0.48      , 0.5       , 0.5       , 0.5       ,\n",
       "        0.3671875 , 0.        ],\n",
       "       [0.66      , 0.76      , 0.75      , 0.75      , 0.875     ,\n",
       "        0.75390625, 1.        ],\n",
       "       [0.5       , 0.64      , 0.25      , 0.625     , 0.5       ,\n",
       "        0.453125  , 1.        ],\n",
       "       [0.34      , 0.32      , 0.5       , 0.5       , 0.5       ,\n",
       "        0.37890625, 0.        ],\n",
       "       [0.66      , 0.64      , 0.5       , 0.75      , 0.625     ,\n",
       "        0.703125  , 1.        ],\n",
       "       [0.28      , 0.24      , 0.25      , 0.375     , 0.625     ,\n",
       "        0.30078125, 0.        ],\n",
       "       [0.82      , 0.88      , 0.75      , 0.875     , 0.875     ,\n",
       "        0.8359375 , 1.        ],\n",
       "       [0.52      , 0.16      , 0.        , 0.125     , 0.25      ,\n",
       "        0.05078125, 0.        ],\n",
       "       [0.72      , 0.72      , 0.5       , 0.625     , 0.5       ,\n",
       "        0.703125  , 1.        ],\n",
       "       [0.6       , 0.28      , 0.25      , 0.375     , 0.5       ,\n",
       "        0.515625  , 0.        ],\n",
       "       [0.48      , 0.36      , 0.25      , 0.25      , 0.5       ,\n",
       "        0.35546875, 0.        ],\n",
       "       [0.5       , 0.44      , 0.25      , 0.5       , 0.5       ,\n",
       "        0.40625   , 0.        ],\n",
       "       [0.88      , 0.88      , 0.75      , 0.75      , 0.625     ,\n",
       "        0.875     , 1.        ],\n",
       "       [0.18      , 0.24      , 0.5       , 0.25      , 0.25      ,\n",
       "        0.28125   , 0.        ],\n",
       "       [0.42      , 0.4       , 0.5       , 0.75      , 0.625     ,\n",
       "        0.32421875, 1.        ],\n",
       "       [0.56      , 0.28      , 1.        , 0.625     , 1.        ,\n",
       "        0.578125  , 1.        ],\n",
       "       [0.86      , 1.        , 1.        , 1.        , 0.875     ,\n",
       "        0.96875   , 1.        ],\n",
       "       [0.74      , 0.76      , 0.5       , 0.625     , 0.5       ,\n",
       "        0.53125   , 1.        ],\n",
       "       [0.14      , 0.08      , 0.25      , 0.375     , 0.125     ,\n",
       "        0.23046875, 0.        ],\n",
       "       [0.54      , 0.24      , 0.25      , 0.5       , 0.375     ,\n",
       "        0.49609375, 0.        ],\n",
       "       [0.5       , 0.4       , 0.5       , 0.75      , 0.375     ,\n",
       "        0.3125    , 0.        ],\n",
       "       [0.64      , 0.36      , 0.75      , 0.5       , 0.375     ,\n",
       "        0.28125   , 1.        ],\n",
       "       [0.48      , 0.48      , 0.25      , 0.75      , 0.625     ,\n",
       "        0.37109375, 0.        ],\n",
       "       [0.78      , 0.68      , 0.75      , 0.875     , 0.75      ,\n",
       "        0.66796875, 1.        ],\n",
       "       [0.82      , 0.92      , 0.75      , 0.875     , 1.        ,\n",
       "        0.828125  , 1.        ],\n",
       "       [0.8       , 0.84      , 1.        , 0.875     , 0.5       ,\n",
       "        0.796875  , 1.        ],\n",
       "       [0.36      , 0.32      , 0.25      , 0.25      , 0.625     ,\n",
       "        0.265625  , 1.        ],\n",
       "       [0.5       , 0.28      , 0.5       , 0.625     , 0.875     ,\n",
       "        0.71484375, 0.        ],\n",
       "       [0.28      , 0.44      , 0.25      , 0.5       , 0.5       ,\n",
       "        0.3515625 , 1.        ],\n",
       "       [0.44      , 0.52      , 0.75      , 0.875     , 0.75      ,\n",
       "        0.52734375, 1.        ],\n",
       "       [0.38      , 0.48      , 0.25      , 0.375     , 0.375     ,\n",
       "        0.2734375 , 0.        ],\n",
       "       [0.58      , 0.48      , 0.5       , 0.75      , 0.5       ,\n",
       "        0.2734375 , 1.        ],\n",
       "       [0.56      , 0.52      , 0.5       , 0.5       , 0.625     ,\n",
       "        0.37890625, 1.        ],\n",
       "       [0.18      , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.015625  , 0.        ],\n",
       "       [0.58      , 0.36      , 0.75      , 0.875     , 0.625     ,\n",
       "        0.53125   , 0.        ],\n",
       "       [0.68      , 0.24      , 0.5       , 0.75      , 1.        ,\n",
       "        0.5234375 , 1.        ],\n",
       "       [0.8       , 0.8       , 0.75      , 0.875     , 0.5       ,\n",
       "        0.73046875, 1.        ],\n",
       "       [0.9       , 0.92      , 1.        , 1.        , 1.        ,\n",
       "        0.984375  , 1.        ],\n",
       "       [0.36      , 0.4       , 0.25      , 0.375     , 0.5       ,\n",
       "        0.30078125, 0.        ],\n",
       "       [0.6       , 0.56      , 0.5       , 0.625     , 0.75      ,\n",
       "        0.4453125 , 1.        ],\n",
       "       [0.68      , 0.52      , 1.        , 0.625     , 0.75      ,\n",
       "        0.53125   , 1.        ],\n",
       "       [1.        , 0.72      , 0.75      , 1.        , 0.875     ,\n",
       "        0.921875  , 1.        ],\n",
       "       [0.56      , 0.6       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.46875   , 0.        ],\n",
       "       [0.3       , 0.32      , 0.25      , 0.25      , 0.375     ,\n",
       "        0.34375   , 0.        ],\n",
       "       [0.5       , 0.4       , 0.5       , 0.5       , 0.375     ,\n",
       "        0.40234375, 0.        ],\n",
       "       [0.72      , 0.8       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.70703125, 1.        ],\n",
       "       [0.4       , 0.2       , 0.25      , 0.125     , 0.25      ,\n",
       "        0.        , 0.        ],\n",
       "       [0.96      , 0.92      , 0.75      , 0.625     , 0.875     ,\n",
       "        0.84375   , 1.        ],\n",
       "       [0.28      , 0.52      , 0.5       , 0.625     , 0.5       ,\n",
       "        0.21875   , 0.        ],\n",
       "       [0.84      , 0.56      , 1.        , 0.875     , 0.75      ,\n",
       "        0.671875  , 1.        ],\n",
       "       [0.66      , 0.68      , 1.        , 0.75      , 1.        ,\n",
       "        1.        , 1.        ],\n",
       "       [0.6       , 0.4       , 0.5       , 0.5       , 0.375     ,\n",
       "        0.49609375, 1.        ],\n",
       "       [0.48      , 0.6       , 0.75      , 0.625     , 0.75      ,\n",
       "        0.57421875, 1.        ],\n",
       "       [0.74      , 0.56      , 1.        , 1.        , 0.625     ,\n",
       "        0.71484375, 1.        ],\n",
       "       [0.        , 0.4       , 0.75      , 0.25      , 0.375     ,\n",
       "        0.0625    , 0.        ],\n",
       "       [0.9       , 0.96      , 1.        , 0.875     , 0.625     ,\n",
       "        0.8359375 , 1.        ],\n",
       "       [0.74      , 0.48      , 0.75      , 0.75      , 0.875     ,\n",
       "        0.56640625, 1.        ],\n",
       "       [0.64      , 0.64      , 1.        , 0.875     , 0.75      ,\n",
       "        0.65234375, 0.        ],\n",
       "       [0.36      , 0.6       , 0.25      , 0.5       , 0.75      ,\n",
       "        0.44921875, 0.        ],\n",
       "       [0.68      , 0.76      , 1.        , 0.75      , 1.        ,\n",
       "        0.76171875, 1.        ],\n",
       "       [0.82      , 0.88      , 1.        , 0.75      , 0.75      ,\n",
       "        0.765625  , 1.        ],\n",
       "       [0.14      , 0.2       , 0.75      , 0.5       , 0.625     ,\n",
       "        0.19921875, 0.        ],\n",
       "       [0.28      , 0.4       , 0.5       , 0.375     , 0.25      ,\n",
       "        0.3203125 , 0.        ],\n",
       "       [0.38      , 0.44      , 0.75      , 0.625     , 0.25      ,\n",
       "        0.34375   , 0.        ],\n",
       "       [0.62      , 0.32      , 0.5       , 0.625     , 0.75      ,\n",
       "        0.66796875, 1.        ],\n",
       "       [0.52      , 0.36      , 0.5       , 0.625     , 0.25      ,\n",
       "        0.1484375 , 0.        ],\n",
       "       [0.62      , 0.6       , 0.5       , 0.5       , 0.75      ,\n",
       "        0.3515625 , 1.        ],\n",
       "       [0.64      , 0.72      , 0.75      , 0.625     , 0.375     ,\n",
       "        0.671875  , 1.        ],\n",
       "       [0.74      , 0.68      , 0.75      , 0.75      , 0.875     ,\n",
       "        0.6640625 , 1.        ],\n",
       "       [0.64      , 0.8       , 1.        , 0.875     , 0.75      ,\n",
       "        0.640625  , 1.        ],\n",
       "       [0.16      , 0.16      , 0.25      , 0.75      , 0.5       ,\n",
       "        0.28515625, 0.        ],\n",
       "       [0.52      , 0.64      , 0.5       , 0.625     , 0.75      ,\n",
       "        0.4921875 , 0.        ],\n",
       "       [0.3       , 0.56      , 1.        , 0.5       , 0.5       ,\n",
       "        0.4609375 , 0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#creating hidden layer with 7 input dimension\n",
    "model.add(Dense(7, activation='relu', input_dim=7))\n",
    "\n",
    "# creating output layer with the linear activation as it is a regression problem\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 7)                 56        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64 (256.00 Byte)\n",
      "Trainable params: 64 (256.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer= 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.2190 - val_loss: 0.1957\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1636 - val_loss: 0.1420\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1168 - val_loss: 0.0999\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0803 - val_loss: 0.0672\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.0434\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0275\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0198 - val_loss: 0.0182\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0132 - val_loss: 0.0140\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0107 - val_loss: 0.0124\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0100 - val_loss: 0.0117\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6850376 ],\n",
       "       [0.6595203 ],\n",
       "       [1.1667035 ],\n",
       "       [0.66300327],\n",
       "       [0.69424903],\n",
       "       [0.5971305 ],\n",
       "       [0.64020926],\n",
       "       [0.6059281 ],\n",
       "       [0.728655  ],\n",
       "       [0.6323513 ],\n",
       "       [0.7264938 ],\n",
       "       [0.612036  ],\n",
       "       [0.72535306],\n",
       "       [0.9828887 ],\n",
       "       [0.69834214],\n",
       "       [0.8281429 ],\n",
       "       [0.6170445 ],\n",
       "       [0.79781324],\n",
       "       [0.86916554],\n",
       "       [0.63538   ],\n",
       "       [0.53276926],\n",
       "       [0.66887605],\n",
       "       [0.71067566],\n",
       "       [0.87037057],\n",
       "       [0.87090594],\n",
       "       [0.58283   ],\n",
       "       [0.9753062 ],\n",
       "       [0.6034227 ],\n",
       "       [0.82778823],\n",
       "       [0.7143325 ],\n",
       "       [0.581398  ],\n",
       "       [0.696048  ],\n",
       "       [0.49902365],\n",
       "       [0.91203314],\n",
       "       [0.46345738],\n",
       "       [0.77317196],\n",
       "       [0.6730955 ],\n",
       "       [0.6174867 ],\n",
       "       [0.6863488 ],\n",
       "       [0.9065312 ],\n",
       "       [0.48868743],\n",
       "       [0.6223189 ],\n",
       "       [0.88452923],\n",
       "       [0.9831379 ],\n",
       "       [0.7979756 ],\n",
       "       [0.39438987],\n",
       "       [0.61948156],\n",
       "       [0.62182564],\n",
       "       [0.72990894],\n",
       "       [0.70151496],\n",
       "       [0.739453  ],\n",
       "       [0.9592323 ],\n",
       "       [0.7814734 ],\n",
       "       [0.62667954],\n",
       "       [0.6823727 ],\n",
       "       [0.6110309 ],\n",
       "       [0.65320164],\n",
       "       [0.61074567],\n",
       "       [0.6326275 ],\n",
       "       [0.682534  ],\n",
       "       [0.26537782],\n",
       "       [0.7447122 ],\n",
       "       [0.6006624 ],\n",
       "       [0.7558677 ],\n",
       "       [1.0065244 ],\n",
       "       [0.5813682 ],\n",
       "       [0.6743518 ],\n",
       "       [0.7782987 ],\n",
       "       [0.9022447 ],\n",
       "       [0.7527853 ],\n",
       "       [0.5238338 ],\n",
       "       [0.59998393],\n",
       "       [0.8288569 ],\n",
       "       [0.37894106],\n",
       "       [1.0402607 ],\n",
       "       [0.6547628 ],\n",
       "       [0.7295855 ],\n",
       "       [0.9189897 ],\n",
       "       [0.65628624],\n",
       "       [0.657113  ],\n",
       "       [0.6715863 ],\n",
       "       [0.72137874],\n",
       "       [0.9030881 ],\n",
       "       [0.6921578 ],\n",
       "       [0.87102544],\n",
       "       [0.7525052 ],\n",
       "       [0.8793668 ],\n",
       "       [0.8967597 ],\n",
       "       [0.7779409 ],\n",
       "       [0.5258701 ],\n",
       "       [0.65779036],\n",
       "       [0.6203308 ],\n",
       "       [0.5816498 ],\n",
       "       [0.7215682 ],\n",
       "       [0.7442756 ],\n",
       "       [0.79522365],\n",
       "       [0.7457093 ],\n",
       "       [0.5516759 ],\n",
       "       [0.7873929 ],\n",
       "       [0.8226213 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting predictions\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30510615767913596"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very poor r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#creating hidden layer with 7 input dimension\n",
    "model.add(Dense(7, activation='relu', input_dim=7))\n",
    "\n",
    "# adding one more hidden layer\n",
    "model.add(Dense(7, activation='relu'))\n",
    "\n",
    "# creating output layer with the linear activation as it is a regression problem\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 7)                 56        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 7)                 56        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120 (480.00 Byte)\n",
      "Trainable params: 120 (480.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer= 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.4109 - val_loss: 0.4099\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3056 - val_loss: 0.3070\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2344 - val_loss: 0.2356\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1785 - val_loss: 0.1791\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1328 - val_loss: 0.1312\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0951 - val_loss: 0.0909\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.0600\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.0371\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0221\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0158 - val_loss: 0.0133\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0092\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0101 - val_loss: 0.0077\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0096 - val_loss: 0.0073\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0095 - val_loss: 0.0071\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0094 - val_loss: 0.0070\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0092 - val_loss: 0.0070\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0090 - val_loss: 0.0069\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0088 - val_loss: 0.0067\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0086 - val_loss: 0.0065\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0085 - val_loss: 0.0064\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0083 - val_loss: 0.0063\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0082 - val_loss: 0.0062\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0061\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0079 - val_loss: 0.0060\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0077 - val_loss: 0.0059\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0076 - val_loss: 0.0058\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0075 - val_loss: 0.0056\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0073 - val_loss: 0.0055\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0072 - val_loss: 0.0054\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0070 - val_loss: 0.0053\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0052\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0068 - val_loss: 0.0050\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0050\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0065 - val_loss: 0.0049\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0064 - val_loss: 0.0049\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0064 - val_loss: 0.0049\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0063 - val_loss: 0.0048\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0062 - val_loss: 0.0047\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0047\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0046\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0060 - val_loss: 0.0047\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0060 - val_loss: 0.0046\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0046\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0046\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0058 - val_loss: 0.0045\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0058 - val_loss: 0.0044\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0044\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0044\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0056 - val_loss: 0.0044\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0056 - val_loss: 0.0043\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0056 - val_loss: 0.0043\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0055 - val_loss: 0.0043\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0055 - val_loss: 0.0042\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.0042\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0054 - val_loss: 0.0042\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.0042\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0042\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0041\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0041\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0052 - val_loss: 0.0041\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0052 - val_loss: 0.0041\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0052 - val_loss: 0.0041\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0052 - val_loss: 0.0041\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0040\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0040\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0040\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0040\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0040\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0040\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0040\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0040\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0039\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0040\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0039\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0039\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0039\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0039\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0039\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0048 - val_loss: 0.0039\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0038\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0038\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0038\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0038\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0038\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0038\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0038\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0038\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0038\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0038\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0038\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0037\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0037\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0037\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0037\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0037\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0037\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0037\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0037\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 890us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6069541 ],\n",
       "       [0.6835621 ],\n",
       "       [0.9509176 ],\n",
       "       [0.73654985],\n",
       "       [0.81451017],\n",
       "       [0.6200039 ],\n",
       "       [0.68715554],\n",
       "       [0.7329646 ],\n",
       "       [0.77845746],\n",
       "       [0.75086343],\n",
       "       [0.60304344],\n",
       "       [0.58933866],\n",
       "       [0.69236314],\n",
       "       [0.7682198 ],\n",
       "       [0.7529947 ],\n",
       "       [0.87104744],\n",
       "       [0.58712643],\n",
       "       [0.7809958 ],\n",
       "       [0.88978267],\n",
       "       [0.58709246],\n",
       "       [0.68541306],\n",
       "       [0.80138093],\n",
       "       [0.8172757 ],\n",
       "       [0.6068385 ],\n",
       "       [0.7522166 ],\n",
       "       [0.48079833],\n",
       "       [0.9055505 ],\n",
       "       [0.60397613],\n",
       "       [0.856117  ],\n",
       "       [0.7305412 ],\n",
       "       [0.5691529 ],\n",
       "       [0.8065913 ],\n",
       "       [0.51473236],\n",
       "       [0.91671616],\n",
       "       [0.37164357],\n",
       "       [0.8029615 ],\n",
       "       [0.6103661 ],\n",
       "       [0.5369356 ],\n",
       "       [0.6027596 ],\n",
       "       [0.90698886],\n",
       "       [0.49449635],\n",
       "       [0.66718894],\n",
       "       [0.69798857],\n",
       "       [0.99961346],\n",
       "       [0.7722165 ],\n",
       "       [0.45143685],\n",
       "       [0.6168696 ],\n",
       "       [0.5873869 ],\n",
       "       [0.593718  ],\n",
       "       [0.63364655],\n",
       "       [0.8268809 ],\n",
       "       [0.9285667 ],\n",
       "       [0.8897974 ],\n",
       "       [0.5919034 ],\n",
       "       [0.76419103],\n",
       "       [0.6448692 ],\n",
       "       [0.7622484 ],\n",
       "       [0.5563513 ],\n",
       "       [0.6719356 ],\n",
       "       [0.6687034 ],\n",
       "       [0.31525025],\n",
       "       [0.6636777 ],\n",
       "       [0.75474966],\n",
       "       [0.8583686 ],\n",
       "       [0.98304164],\n",
       "       [0.54865795],\n",
       "       [0.718372  ],\n",
       "       [0.72541344],\n",
       "       [0.916506  ],\n",
       "       [0.6588384 ],\n",
       "       [0.5290225 ],\n",
       "       [0.5874065 ],\n",
       "       [0.8126737 ],\n",
       "       [0.3653943 ],\n",
       "       [0.89444673],\n",
       "       [0.58255994],\n",
       "       [0.8011909 ],\n",
       "       [0.9089208 ],\n",
       "       [0.6609006 ],\n",
       "       [0.75788814],\n",
       "       [0.8174151 ],\n",
       "       [0.4333797 ],\n",
       "       [0.93086386],\n",
       "       [0.74702865],\n",
       "       [0.7713059 ],\n",
       "       [0.66647524],\n",
       "       [0.86335856],\n",
       "       [0.8799594 ],\n",
       "       [0.48873904],\n",
       "       [0.55630696],\n",
       "       [0.5952658 ],\n",
       "       [0.7168237 ],\n",
       "       [0.50503993],\n",
       "       [0.692486  ],\n",
       "       [0.7968118 ],\n",
       "       [0.8150894 ],\n",
       "       [0.84924906],\n",
       "       [0.5379408 ],\n",
       "       [0.69717   ],\n",
       "       [0.604889  ]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting predictions\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7301373774768125"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now r2_score has improved a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], color='red', label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], color='blue', label='Validation Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.4108981490135193,\n",
       "  0.30559712648391724,\n",
       "  0.2344210147857666,\n",
       "  0.1785200983285904,\n",
       "  0.13278043270111084,\n",
       "  0.0950714498758316,\n",
       "  0.06364859640598297,\n",
       "  0.04066672921180725,\n",
       "  0.02494095079600811,\n",
       "  0.01575453020632267,\n",
       "  0.011472100391983986,\n",
       "  0.010070031508803368,\n",
       "  0.009636109694838524,\n",
       "  0.009540123865008354,\n",
       "  0.009352527558803558,\n",
       "  0.009187817573547363,\n",
       "  0.008962534368038177,\n",
       "  0.008794784545898438,\n",
       "  0.008627844043076038,\n",
       "  0.008465618826448917,\n",
       "  0.008319552056491375,\n",
       "  0.008155311457812786,\n",
       "  0.00802623201161623,\n",
       "  0.007876647636294365,\n",
       "  0.007739773951470852,\n",
       "  0.007597879972308874,\n",
       "  0.007466659881174564,\n",
       "  0.00730418274179101,\n",
       "  0.0071659991517663,\n",
       "  0.007038732524961233,\n",
       "  0.006884853355586529,\n",
       "  0.006757439114153385,\n",
       "  0.006649032235145569,\n",
       "  0.006549750920385122,\n",
       "  0.006447954569011927,\n",
       "  0.006364504806697369,\n",
       "  0.006284081377089024,\n",
       "  0.006211438681930304,\n",
       "  0.006140512879937887,\n",
       "  0.006081833504140377,\n",
       "  0.006009464152157307,\n",
       "  0.005975095089524984,\n",
       "  0.005894904024899006,\n",
       "  0.005853045731782913,\n",
       "  0.005799335893243551,\n",
       "  0.005761520471423864,\n",
       "  0.005711446050554514,\n",
       "  0.005666675511747599,\n",
       "  0.00564446859061718,\n",
       "  0.005595671944320202,\n",
       "  0.005565060768276453,\n",
       "  0.005512821022421122,\n",
       "  0.005477640777826309,\n",
       "  0.005445279646664858,\n",
       "  0.005407626274973154,\n",
       "  0.00537389749661088,\n",
       "  0.005341491661965847,\n",
       "  0.005309007130563259,\n",
       "  0.005273687187582254,\n",
       "  0.005244266241788864,\n",
       "  0.005203775595873594,\n",
       "  0.005179078783839941,\n",
       "  0.005152967758476734,\n",
       "  0.005123412702232599,\n",
       "  0.005103301256895065,\n",
       "  0.00507371686398983,\n",
       "  0.005047026555985212,\n",
       "  0.005006421357393265,\n",
       "  0.0049904873594641685,\n",
       "  0.004959932994097471,\n",
       "  0.0049360161647200584,\n",
       "  0.004944095853716135,\n",
       "  0.004909420385956764,\n",
       "  0.004871502052992582,\n",
       "  0.004848832730203867,\n",
       "  0.004837139043956995,\n",
       "  0.004800976719707251,\n",
       "  0.004785075783729553,\n",
       "  0.004771488253027201,\n",
       "  0.004747704137116671,\n",
       "  0.004720429889857769,\n",
       "  0.004722018260508776,\n",
       "  0.004670499358326197,\n",
       "  0.004672808572649956,\n",
       "  0.004651570692658424,\n",
       "  0.004653813783079386,\n",
       "  0.004613875411450863,\n",
       "  0.00462340097874403,\n",
       "  0.0045877546072006226,\n",
       "  0.0045758746564388275,\n",
       "  0.004550812300294638,\n",
       "  0.004578443709760904,\n",
       "  0.004494879394769669,\n",
       "  0.0045076943933963776,\n",
       "  0.004475564230233431,\n",
       "  0.004470410756766796,\n",
       "  0.004443817306309938,\n",
       "  0.004425705410540104,\n",
       "  0.00442215520888567,\n",
       "  0.004402925726026297],\n",
       " 'val_loss': [0.40991324186325073,\n",
       "  0.3070264160633087,\n",
       "  0.23560872673988342,\n",
       "  0.17906172573566437,\n",
       "  0.13120608031749725,\n",
       "  0.09091910719871521,\n",
       "  0.06000388786196709,\n",
       "  0.037106793373823166,\n",
       "  0.022085797041654587,\n",
       "  0.013340351171791553,\n",
       "  0.009175624698400497,\n",
       "  0.0077146561816334724,\n",
       "  0.007327146828174591,\n",
       "  0.007131093647330999,\n",
       "  0.007027526386082172,\n",
       "  0.0069946954026818275,\n",
       "  0.006854601204395294,\n",
       "  0.00668198149651289,\n",
       "  0.006533435080200434,\n",
       "  0.006421704776585102,\n",
       "  0.00628745649009943,\n",
       "  0.006194539833813906,\n",
       "  0.006101328879594803,\n",
       "  0.005953653249889612,\n",
       "  0.005883457604795694,\n",
       "  0.005765383597463369,\n",
       "  0.005581751000136137,\n",
       "  0.005466134287416935,\n",
       "  0.005350298248231411,\n",
       "  0.005318299867212772,\n",
       "  0.0051545267924666405,\n",
       "  0.005021912045776844,\n",
       "  0.004991787485778332,\n",
       "  0.004928744863718748,\n",
       "  0.004942330066114664,\n",
       "  0.004894942510873079,\n",
       "  0.004841793794184923,\n",
       "  0.004686414729803801,\n",
       "  0.004666781984269619,\n",
       "  0.004649366717785597,\n",
       "  0.004713139496743679,\n",
       "  0.004586067982017994,\n",
       "  0.004552086349576712,\n",
       "  0.004554368555545807,\n",
       "  0.004521267954260111,\n",
       "  0.004428955260664225,\n",
       "  0.004435946233570576,\n",
       "  0.004432524088770151,\n",
       "  0.004396190866827965,\n",
       "  0.004295909311622381,\n",
       "  0.004341474734246731,\n",
       "  0.00429536122828722,\n",
       "  0.004244965501129627,\n",
       "  0.004229395650327206,\n",
       "  0.004210834391415119,\n",
       "  0.004202661570161581,\n",
       "  0.0041838050819933414,\n",
       "  0.004128069616854191,\n",
       "  0.004147922620177269,\n",
       "  0.004099692218005657,\n",
       "  0.004092310555279255,\n",
       "  0.004109809175133705,\n",
       "  0.0040671974420547485,\n",
       "  0.0040498957969248295,\n",
       "  0.004018631298094988,\n",
       "  0.004019615240395069,\n",
       "  0.004002690315246582,\n",
       "  0.004011765122413635,\n",
       "  0.003992222715169191,\n",
       "  0.003969920799136162,\n",
       "  0.003960444591939449,\n",
       "  0.003902983386069536,\n",
       "  0.004009411670267582,\n",
       "  0.003922202158719301,\n",
       "  0.003872433677315712,\n",
       "  0.003911125939339399,\n",
       "  0.0038939309306442738,\n",
       "  0.0038615979719907045,\n",
       "  0.0038975973147898912,\n",
       "  0.003823342500254512,\n",
       "  0.0038381728809326887,\n",
       "  0.003833400085568428,\n",
       "  0.003802965162321925,\n",
       "  0.003798471298068762,\n",
       "  0.0038446877151727676,\n",
       "  0.0037676100619137287,\n",
       "  0.0037793393712490797,\n",
       "  0.003873247653245926,\n",
       "  0.0037583441007882357,\n",
       "  0.003792688250541687,\n",
       "  0.0037393257953226566,\n",
       "  0.003800757694989443,\n",
       "  0.0037214779295027256,\n",
       "  0.003728174837306142,\n",
       "  0.0037202518433332443,\n",
       "  0.003723581787198782,\n",
       "  0.003729320364072919,\n",
       "  0.003702374640852213,\n",
       "  0.0037204984109848738,\n",
       "  0.003697561100125313]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history  # dictionary with keywords - loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
